{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Tabular Prior-Data Fitted Network (TapPFN) for Structural Damage Prediction"
      ],
      "metadata": {
        "id": "0yNZWiuEuMkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (if necessary, typically run once)\n",
        "# !pip install tabpfn pandas numpy seaborn matplotlib scikit-learn xgboost\n",
        "\n",
        "# Clone and install the repository for TabPFN +10 classes\n",
        "# !pip install \"tabpfn-extensions[all] @ git+https://github.com/PriorLabs/tabpfn-extensions.git\""
      ],
      "metadata": {
        "id": "iLP5xv7Mt2Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from tabpfn import TabPFNClassifier\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "import torch # TabPFN uses torch"
      ],
      "metadata": {
        "id": "NPnpVYcCt-fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup device for TabPFN"
      ],
      "metadata": {
        "id": "d5wVb_icuIwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "a1uZG7AHuCZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading and normalization"
      ],
      "metadata": {
        "id": "T-D0gmzjugJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fname = join(getcwd(),'data3SS2009.mat')\n",
        "mat_contents = sio.loadmat(fname)\n",
        "dataset = mat_contents['dataset']\n",
        "N, Chno, Nc = dataset.shape\n",
        "\n",
        "print(f'''\n",
        "Number of samples (N): {N}\n",
        "Number of channels (Chno): {Chno}\n",
        "Number of cases (Nc): {Nc}\n",
        "''')\n",
        "\n",
        "y = mat_contents['labels'].reshape(Nc)\n",
        "y_target = y\n",
        "\n",
        "for i in range(len(y_target)):\n",
        "    y_target[i] -= 1\n",
        "\n",
        "\n",
        "Ch1 = dataset[:,0,:] # load cell: shaker force\n",
        "Ch2 = dataset[:,1,:] # accelerometer: base\n",
        "Ch3 = dataset[:,2,:] # accelerometer: 1st floor\n",
        "Ch4 = dataset[:,3,:] # accelerometer: 2nd floor\n",
        "Ch5 = dataset[:,4,:] # accelerometer: 3rd floor\n",
        "\n",
        "Ts = 3.125 * 1e-3 # sampling time\n",
        "time = (np.linspace(1,N,N) - 1) * Ts\n",
        "\n",
        "na = 30\n",
        "\n",
        "FeatARCh2 = []\n",
        "FeatARCh3 = []\n",
        "FeatARCh4 = []\n",
        "FeatARCh5 = []\n",
        "\n",
        "for i in tqdm(range(Nc)):\n",
        "  res1 = AutoReg(Ch2[:,i], lags = na, trend = 'n').fit()\n",
        "  res2 = AutoReg(Ch3[:,i], lags = na, trend = 'n').fit()\n",
        "  res3 = AutoReg(Ch4[:,i], lags = na, trend = 'n').fit()\n",
        "  res4 = AutoReg(Ch5[:,i], lags = na, trend = 'n').fit()\n",
        "\n",
        "  FeatARCh2.append(res1.params)\n",
        "  FeatARCh3.append(res2.params)\n",
        "  FeatARCh4.append(res3.params)\n",
        "  FeatARCh5.append(res4.params)\n",
        "\n",
        "FeatARCh2 = np.array(FeatARCh2)\n",
        "FeatARCh3 = np.array(FeatARCh3)\n",
        "FeatARCh4 = np.array(FeatARCh4)\n",
        "FeatARCh5 = np.array(FeatARCh5)\n",
        "\n",
        "X1 = np.concatenate([FeatARCh2,FeatARCh3,FeatARCh4,FeatARCh5], axis = 1)\n",
        "\n",
        "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
        "indDam = y > 9\n",
        "dfARred = pd.concat([pd.DataFrame(scaler.fit_transform(X1)), pd.DataFrame({'target':indDam})],axis=1)\n",
        "dfARred.describe()"
      ],
      "metadata": {
        "id": "C3pAmgbCu-JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data visualization"
      ],
      "metadata": {
        "id": "L6Ni5Wg7wPl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfARred_for_plot = dfARred.copy()\n",
        "dfARred_for_plot['target'] = dfARred_for_plot['target'].map({False: 'Undamaged', True: 'Damaged'})\n",
        "num_features = len(dfARred_for_plot.columns) - 1\n",
        "plt.figure(figsize=[20, 7])\n",
        "ax_top = plt.gca()\n",
        "pd.plotting.parallel_coordinates(\n",
        "    dfARred_for_plot,\n",
        "    'target',\n",
        "    colormap=plt.get_cmap('Paired'),\n",
        "    ax=ax_top,\n",
        "    axvlines=False\n",
        ")\n",
        "ax_top.set_xticks(range(0, num_features, 10))\n",
        "ax_top.set_xticklabels(ax_top.get_xticks(), rotation=90)\n",
        "plt.xlabel('AR Coefficient Index')\n",
        "plt.ylabel('Normalized Value')\n",
        "plt.title('Parallel Coordinates: AR Coefficients by Damage State')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.figure(figsize=[15, 6])\n",
        "plt.subplot(121)\n",
        "ax_left = plt.gca()\n",
        "pd.plotting.parallel_coordinates(\n",
        "    dfARred_for_plot[dfARred['target'] == False],\n",
        "    'target',\n",
        "    colormap=plt.get_cmap('Paired'),\n",
        "    ax=ax_left,\n",
        "    axvlines=False\n",
        ")\n",
        "plt.xlabel('AR Coefficient Index')\n",
        "plt.ylabel('Normalized Value')\n",
        "plt.title('Parallel Coordinates: Undamaged Cases', y=1.02)\n",
        "\n",
        "ax_left.set_xticks(range(0, num_features, 10))\n",
        "ax_left.set_xticklabels(ax_left.get_xticks(), rotation=90)\n",
        "plt.subplot(122)\n",
        "ax_right = plt.gca()\n",
        "pd.plotting.parallel_coordinates(\n",
        "    dfARred_for_plot[dfARred['target'] == True],\n",
        "    'target',\n",
        "    colormap=plt.get_cmap('Paired'),\n",
        "    ax=ax_right,\n",
        "    axvlines=False\n",
        ")\n",
        "plt.xlabel('AR Coefficient Index')\n",
        "plt.title('Parallel Coordinates: Damaged Cases', y=1.02)\n",
        "ax_right.set_xticks(range(0, num_features, 10))\n",
        "ax_right.set_xticklabels(ax_right.get_xticks(), rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b0u3oSJ4wX-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indTest = [200, 600, 800]\n",
        "nSpect = len(indTest)\n",
        "Ts = 3.125 * 1e-3\n",
        "Fs = 1 / Ts\n",
        "channel_groups_data = [\n",
        "    ([Ch2, Ch3], ['Ch2 (Base)', 'Ch3 (1st Floor)']),\n",
        "    ([Ch4, Ch5], ['Ch4 (2nd Floor)', 'Ch5 (3rd Floor)'])\n",
        "]\n",
        "for group_idx, (current_channels, current_channel_names) in enumerate(channel_groups_data):\n",
        "    num_channels_in_group = len(current_channels)\n",
        "    fig, axTuple = plt.subplots(nrows=nSpect, ncols=num_channels_in_group,\n",
        "                                figsize=(20, 12),\n",
        "                                sharex='col', sharey='row',\n",
        "                                gridspec_kw={'wspace': 0.4, 'hspace': 0.4})\n",
        "    suptitle_text = f'Spectrograms for {current_channel_names[0]} and {current_channel_names[1]}'\n",
        "    fig.suptitle(suptitle_text, fontsize=18)\n",
        "    ims_for_colorbars = [None] * num_channels_in_group\n",
        "    for col_idx, (channel_data, channel_name) in enumerate(zip(current_channels, current_channel_names)):\n",
        "        for row_idx, time_point_idx in enumerate(indTest):\n",
        "            current_ax = axTuple[row_idx, col_idx]\n",
        "            x = channel_data[:, time_point_idx]\n",
        "            Pxx, freqs, bins, im = current_ax.specgram(x, Fs=Fs)\n",
        "            if row_idx == nSpect - 1:\n",
        "                ims_for_colorbars[col_idx] = im\n",
        "            current_ax.set_title(f'{channel_name} - Time Point: {time_point_idx}', fontsize=12)\n",
        "            current_ax.grid(True)\n",
        "            if row_idx == nSpect - 1:\n",
        "                current_ax.set_xlabel('Time [s]', fontsize=10)\n",
        "            if col_idx == 0:\n",
        "                current_ax.set_ylabel('Frequency [Hz]', fontsize=10)\n",
        "            current_ax.tick_params(axis='both', which='major', labelsize=9)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    bottom_y_overall = axTuple[nSpect-1, 0].get_position().y0\n",
        "    top_y_overall = axTuple[0, 0].get_position().y1\n",
        "    height_overall_subplots = top_y_overall - bottom_y_overall\n",
        "    cbar_ax0 = fig.add_axes([pos_col0_right_edge + 0.02, bottom_y_overall, 0.015, height_overall_subplots])\n",
        "    cbar0 = fig.colorbar(ims_for_colorbars[0], cax=cbar_ax0)\n",
        "    cbar0.set_label('Frequency [Hz]', fontsize=10)\n",
        "    cbar_ax1 = fig.add_axes([pos_col1_right_edge + 0.02, bottom_y_overall, 0.015, height_overall_subplots])\n",
        "    cbar1 = fig.colorbar(ims_for_colorbars[1], cax=cbar_ax1)\n",
        "    cbar1.set_label('Frequency [Hz]', fontsize=10)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "msYC6963wyRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing"
      ],
      "metadata": {
        "id": "YHvb2ut-xMoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quicktest = True\n",
        "if quicktest:\n",
        "    iterTest = 2\n",
        "    kfolds, nk = (4, 7)\n",
        "    n_search = 5\n",
        "else:\n",
        "    iterTest = 15\n",
        "    kfolds, nk = (5, 10)\n",
        "    n_search = 25\n",
        "\n",
        "PredDF = pd.DataFrame(columns=[\"y\", \"yh\", \"feature\", \"model\", \"nholdout\"])\n",
        "ModelsDF = pd.DataFrame(columns=[\"feature\", \"model\", \"nbytes\", \"nholdout\"])\n",
        "TimeDF = pd.DataFrame(columns=[\"feature\", \"model\", \"inference_time\", \"nholdout\"])\n",
        "models = [\n",
        "    (\"TabPFN\", ManyClassClassifier(\n",
        "       estimator=TabPFNClassifier(device='cuda'),\n",
        "       alphabet_size=10,\n",
        "       n_estimators=15\n",
        "    )),\n",
        "    (\"SVC\", SVC(probability=True)),\n",
        "    (\"KNN\", KNeighborsClassifier()),\n",
        "    (\"RandomForest\", RandomForestClassifier()),\n",
        "    (\"XGBoost\", XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'))\n",
        "]\n",
        "param_grids = [\n",
        "    None,  # TabPFN does not require hyperparameter adjustment\n",
        "    {\n",
        "       \"SVC__C\": stats.loguniform(1e-3, 1e3),\n",
        "       \"SVC__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
        "       \"SVC__gamma\": stats.loguniform(1e-4, 1e0)\n",
        "    },\n",
        "    {\n",
        "        \"KNN__n_neighbors\": stats.randint(3, 20),\n",
        "        \"KNN__weights\": [\"uniform\", \"distance\"]\n",
        "    },\n",
        "    {\n",
        "        \"RandomForest__n_estimators\": stats.randint(50, 200),\n",
        "        \"RandomForest__max_depth\": stats.randint(5, 30),\n",
        "        \"RandomForest__min_samples_split\": stats.randint(2, 10)\n",
        "    },\n",
        "    {\n",
        "        \"XGBoost__n_estimators\": stats.randint(50, 200),\n",
        "        \"XGBoost__max_depth\": stats.randint(3, 10),\n",
        "        \"XGBoost__learning_rate\": stats.uniform(0.01, 0.3)\n",
        "    }\n",
        "]\n",
        "\n",
        "featList = [X1]\n",
        "featName = ['X1']\n",
        "Nfeats = len(featList)\n",
        "\n",
        "for k, (model_name, model) in enumerate(models):\n",
        "    for j in range(Nfeats):\n",
        "        X = featList[j]\n",
        "        y = y_target\n",
        "        print(f\"\\nModel: {model_name}\")\n",
        "        for i in tqdm(range(iterTest)):\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.5, stratify=y, random_state=i\n",
        "            )\n",
        "            rkf = RepeatedStratifiedKFold(n_splits=kfolds, n_repeats=nk, random_state=i)\n",
        "            clf = Pipeline([(\"scaler\", StandardScaler()), (model_name, model)])\n",
        "            param_grid = param_grids[k]\n",
        "            if param_grid:\n",
        "                random_search = RandomizedSearchCV(\n",
        "                    clf,\n",
        "                    param_distributions=param_grid,\n",
        "                    n_iter=n_search,\n",
        "                    cv=rkf,\n",
        "                    scoring=\"accuracy\",\n",
        "                    n_jobs=-1,\n",
        "                    random_state=i\n",
        "                )\n",
        "                random_search.fit(X_train, y_train)\n",
        "                best_model = random_search.best_estimator_\n",
        "            else:\n",
        "                clf.fit(X_train, y_train)\n",
        "                best_model = clf\n",
        "            start_time = time.time()\n",
        "            yh_test = best_model.predict(X_test)\n",
        "            inference_time = time.time() - start_time\n",
        "\n",
        "            with open('temp_model.pickle', 'wb') as handle:\n",
        "                pk.dump(best_model, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
        "            nbytes = os.path.getsize('temp_model.pickle')\n",
        "            PredDF = pd.concat([PredDF, pd.DataFrame({\n",
        "                \"y\": y_test,\n",
        "                \"yh\": yh_test,\n",
        "                \"feature\": featName[j],\n",
        "                \"model\": model_name,\n",
        "                \"nholdout\": i\n",
        "            })], ignore_index=True)\n",
        "            ModelsDF = pd.concat([ModelsDF, pd.DataFrame({\n",
        "                \"feature\": [featName[j]],\n",
        "                \"model\": [model_name],\n",
        "                \"nbytes\": [nbytes],\n",
        "                \"nholdout\": [i]\n",
        "            })], ignore_index=True)\n",
        "            TimeDF = pd.concat([TimeDF, pd.DataFrame({\n",
        "                \"feature\": [featName[j]],\n",
        "                \"model\": [model_name],\n",
        "                \"inference_time\": [inference_time],\n",
        "                \"nholdout\": [i]\n",
        "            })], ignore_index=True)\n",
        "\n",
        "\n",
        "with open(f\"results.p\", \"wb\") as fname:\n",
        "    pk.dump([PredDF, ModelsDF, [name for name, _ in models], featName, TimeDF], fname)"
      ],
      "metadata": {
        "id": "MZCJtld1xMAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}